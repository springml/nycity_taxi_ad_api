# PubSub Taxi Rides feeder program written in Go

The emulated data stream generated by this tool is based on the 
[NYC Taxi & Limousine Commission’s open dataset](https://data.cityofnewyork.us/) expanded with additional routing 
information using the [Google Maps Direction API](https://developers.google.com/maps/documentation/directions/) and 
interpolated timestamps to simulate a real time scenario.

## Setup

Uses your gcloud auth context, make sure your current user has access to the project
You might need to do `gcloud auth login` after you switched to the user you want to use.
The program uses default credentials from gcloud sdk env if no `service-account.json` is present.

There is a bug when using gcloud default credentials with high QPS to pubsub push because it uses
a shared quota: see b/30182822

Please use service account credentials for high QPS publishing. Just create a Compute Engine default service account in JSON format
in Google Cloud Developer Console and save it to `service-account.json`.

## Running the program locally

Command is the following (detailed help with `go run *.go --help`):
It's possible to use environment variables, a config `-config properties.env` file or parametes.
```
BUCKET=<BUCKET_NAME> \
PROJECT=<PROJECT_ID> \
FILEPREFIX=<FILE_PREFIX> \
PUBSUBTOPIC=<PUBSUB_TOPIC> \
SPEEDUP=<SPEEDUP_FACTOR> \
SKIPRIDES=<MODULO_N_TO_SKIP_RIDES_AND_LOWER_QPS> \
SKIPFILES=<MODULO_N_TO_SKIP_INPUT_FILES_FOR_SCALEOUT> \
DEBUG=true \
go run *.go
```
Remove `DEBUG=true` if you don't want to get the debug output on stdout

You can also build and run a Docker container
Build the container with `make`. This uses a 2-step process to build a small container based on alpine running the feeder.
To run the container locally create a `properties.env`
```
echo -e "\
BUCKET=<BUCKET_NAME>\n\
PROJECT=<PROJECT_ID>\n\
FILEPREFIX=<FILE_PREFIX>\n\
PUBSUBTOPIC=<PUBSUB_TOPIC>\n\
SPEEDUP=<SPEEDUP_FACTOR>\n\
SKIPRIDES=<MODULO_N_TO_SKIP_RIDES_AND_LOWER_QPS>\n\
SKIPFILES=<MODULO_N_TO_SKIP_INPUT_FILES_FOR_SCALEOUT>\n\
DEBUG=true" > properties.env
```
and then run the container. You'll need a service account if you don't [run it on GCE](#running-the-program-on-gce).
```
docker run --rm -v $PWD/service-account.json:/service-account.json --env-file=properties.env feeder
```

### Example config reading from JSON formatted input:
```
BUCKET=billion-taxi-rides \
PROJECT=billion-taxi-rides \
FILEPREFIX=json/yellow_tripdata_2015-01-ext000 \
PUBSUBTOPIC=realtime-feed \
SPEEDUP=60 \
SKIPRIDES=100 \
SKIPFILES=10 \
DEBUG=true
```

## Running the program on GCE
Build the container and push to [Google Cloud Container Registry](https://cloud.google.com/container-registry/)
with `make push-gcr`. Note: The Makefile default is set to eu.gcr.io which can be changed by setting the `GCR` environment variables
e.g. `GCR=gcr.io make push-gcr`

Create a GCE instance selecting CoreOS stable.
Make sure to give the instance API access to Google Cloud Pubsub and Google Cloud Storage.

Add a startup script
```
echo -e "\
BUCKET=<BUCKET_NAME>\n\
PROJECT=<PROJECT_ID>\n\
FILEPREFIX=<FILE_PREFIX>\n\
PUBSUBTOPIC=<PUBSUB_TOPIC>\n\
SPEEDUP=<SPEEDUP_FACTOR>\n\
SKIPRIDES=<MODULO_N_TO_SKIP_RIDES_AND_LOWER_QPS>\n\
SKIPFILES=<MODULO_N_TO_SKIP_INPUT_FILES_FOR_SCALEOUT>\n\
DEBUG=true" > /root/properties.env

echo -e "\
#/bin/sh!
METADATA=http://metadata.google.internal./computeMetadata/v1\n\
SVC_ACCT=\$METADATA/instance/service-accounts/default\n\
ACCESS_TOKEN=\$(curl -H 'Metadata-Flavor: Google' \$SVC_ACCT/token | cut -d'\"' -f 4)\n\
docker login -e not@val.id -u _token -p \$ACCESS_TOKEN https://eu.gcr.io" > /root/auth-docker.sh
chmod +x /root/auth-docker.sh
```

### ON STARTUP feeding of taxi ride points to pubsub
Add the following lines at the end of the startup script. Replace `<PROJECT_NAME>` 
with your GCP project name where your container registry lives.
```
/root/auth-docker.sh
docker run -d --env-file=/root/properties.env eu.gcr.io/<PROJECT_NAME>/feeder
```

### ON DEMAND feeding of taxi ride points to pubsub
SSH into your instance and execute:
```
sudo su -
./auth-docker.sh
docker pull eu.gcr.io/<PROJECT_NAME>/feeder
docker run -d --env-file=properties.env eu.gcr.io/<PROJECT_NAME>/feeder
```

To stop your container, find the running container id with `docker ps` and run `docker stop <container-id>`

## Filtering invalid data
The source dataset contains several rides with very long durations. To avoid running out of memory
for continuous running feeders we filter those with an upper limit of 5hrs.
You can find those rides by using [BigQuery](https://cloud.google.com/bigquery) on the public
available NYC Taxi Rides dataset.

```
#StandardSQL
SELECT TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, MINUTE) as trip_time, * 
FROM `bigquery-public-data.new_york.tlc_yellow_trips_2015`
WHERE TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, MINUTE) > 300
```

## Vendor Packaging
We use [`govendor`](https://github.com/kardianos/govendor) (`go get -u github.com/kardianos/govendor`) as the vendor package manager.


*Use: The NYC Taxi & Limousine Commission’s dataset is publicly available for anyone to use under the following terms 
provided by the Dataset Source —https://data.cityofnewyork.us/— and is provided "AS IS" without any warranty, express 
or implied, from Google. 
Google disclaims all liability for any damages, direct or indirect, resulting from the use of the dataset.*

*This is not an official Google product*
